{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "from revscoring.features import wikitext\n",
    "from revscoring.datasources.meta import mappers, vectorizers\n",
    "from revscoring.datasources import revision_oriented\n",
    "from revscoring.dependencies import solve\n",
    "from revscoring.features.meta import aggregators\n",
    "\n",
    "import mwxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(indir):\n",
    "    return [os.path.join(indir,f) for f in os.listdir(indir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dump(filepath, save_text=False, save_tokens=False):\n",
    "    with bz2.open(filepath) as filestream:\n",
    "        observation_list = []\n",
    "        dump = mwxml.Dump.from_file(filestream)\n",
    "        for i, page in enumerate(dump):\n",
    "            for rev in page:\n",
    "                observation = {\n",
    "                    'title':page.title,\n",
    "                    'page_id':page.id,\n",
    "                    'rev_id':rev.id,\n",
    "                    'redirect':page.redirect\n",
    "                }\n",
    "                \n",
    "                if save_text:\n",
    "                    observation['text'] = rev.text\n",
    "                    \n",
    "                tokenized_text = tokenize_and_clean_text(rev.text,TOKENIZER)\n",
    "                \n",
    "                if save_tokens:\n",
    "                    observation['tokenized_text'] = tokenized_text\n",
    "                    \n",
    "                observation['feature_vector'] = vectorize_text(observation['tokenized_text'],w2v)\n",
    "                observation_list.append(observation)\n",
    "                \n",
    "    return observation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_clean_text(revision_text,tokenizer):\n",
    "    cache = {}\n",
    "    cache[revision_oriented.revision.text] = revision_text\n",
    "    return solve(tokenizer, cache=cache, context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(tokenized_text,vectorizer):\n",
    "    cache = {}\n",
    "    cache[TOKENIZER] = tokenized_text\n",
    "    return solve(vectorizer, cache=cache, context=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = mappers.lower_case(wikitext.revision.datasources.words)\n",
    "INDIR = '/home/jmads/data/enwiki/'\n",
    "OUTDIR = '/home/jmads/wiki-ltt-cluster/datasets/vectorized/'\n",
    "ENWIKI_KVS = '/home/jmads/wiki-ltt-cluster/word2vec/enwiki-20200501-learned_vectors.50_cell.10k.kv'\n",
    "    \n",
    "enwiki_kvs = vectorizers.word2vec.load_gensim_kv(\n",
    "    path=ENWIKI_KVS,\n",
    "    mmap=\"r\"\n",
    ")\n",
    "\n",
    "def vectorize_words(words):\n",
    "    return vectorizers.word2vec.vectorize_words(enwiki_kvs, words)\n",
    "\n",
    "revision_text_vectors = vectorizers.word2vec(\n",
    "    mappers.lower_case(wikitext.revision.datasources.words),\n",
    "    vectorize_words,\n",
    "    name=\"revision.text.en_vectors\")\n",
    "\n",
    "w2v = aggregators.mean(\n",
    "    revision_text_vectors,\n",
    "    vector=True,\n",
    "    name=\"revision.text.en_vectors_mean\"\n",
    ")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    files_to_process = os.listdir(INDIR)\n",
    "    for infile in files_to_process:\n",
    "        infile_path = os.path.join(INDIR,infile)\n",
    "        page_list = process_dump(infile_path, save_text=True, save_tokens=True)\n",
    "        outfile_name = '{0}.json'.format(infile.split('.')[0])\n",
    "        outfile_path = os.path.join(OUTDIR,outfile_name)\n",
    "        with open(outfile_path,'w') as outfile:\n",
    "            json.dump(page_list,outfile)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-latest-pages-articles1.xml-p1p41242.bz2']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(INDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-latest-pages-articles1', 'xml-p1p41242', 'bz2']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'enwiki-latest-pages-articles1.xml-p1p41242.bz2'.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
